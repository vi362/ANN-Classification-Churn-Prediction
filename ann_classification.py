# -*- coding: utf-8 -*-
"""ANN_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NFTvpqiMFgLl_9qx7L1zvU9z3Fokl-ce
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pickle

data = pd.read_csv("/content/Churn_Modelling.csv")
data.head()

data.shape

data = data.drop(columns = ['RowNumber','CustomerId','Surname'],axis=1)
data.head()

label_encoder_gender = LabelEncoder()
data['Gender'] = label_encoder_gender.fit_transform(data['Gender'])
data.head()

data.tail()

from sklearn.preprocessing import OneHotEncoder
onehot_encoder_geo = OneHotEncoder()
geo_encoder = onehot_encoder_geo.fit_transform(data[['Geography']])

geo_encoder

feature = onehot_encoder_geo.get_feature_names_out(['Geography'])
feature

geo_encoded_df = pd.DataFrame(geo_encoder.toarray(),columns=feature)
geo_encoded_df.head()

# Instead of dropping 'Geography' again, directly concatenate the dataframes:
data = pd.concat([data, geo_encoded_df], axis=1)
data.head()

with open('label_encoder_gender.pkl','wb') as file:
  pickle.dump(label_encoder_gender,file)

with open('onehot_encoder_geo.pkl','wb') as file:
  pickle.dump(onehot_encoder_geo,file)

X = data.drop(columns=['Exited','Geography'])
y = data['Exited']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.head()

y_train.head()

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train_df = pd.DataFrame(X_train)  # Convert X_train back to a DataFrame
X_train_df.head()  # Now you can use head()

with open ('scaler.pkl','wb') as file:
  pickle.dump(scaler,file)

data

"""# **ANN Implementation**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
import datetime

"""Build our ANN Model"""

model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Change input_dim to input_shape
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.summary()

# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=0.01)

loss = tf.keras.losses.BinaryCrossentropy()

model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

#set up tensorboard
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%m%d-%H%M%S")
tensorflow_callbacks = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Setup Early Stopping
early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

# Train the Model
history = model.fit(
    X_train, y_train,validation_data = (X_test,y_test),
    epochs = 100,
    callbacks = [tensorflow_callbacks,early_stop]
)

model.save('model.h5')

# Commented out IPython magic to ensure Python compatibility.
# Load Tensorboard Extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

"""# **Integrating ANN Model with Streamlit Web App**"""

import streamlit as st

!pip install streamlit

# Load the trained model
model = tf.keras.models.load_model('model.h5')

# Load the Encoders and Scaler

with open('label_encoder_gender.pkl','rb') as file:
  label_encoder_gender = pickle.load(file)

with open('onehot_encoder_geo.pkl','rb') as file:
  onehot_encoder_geo = pickle.load(file)

with open('scaler.pkl','rb') as file:
  scaler = pickle.load(file)

"""**Streamlit App**"""

st.title('Customer Churn Prediction')

# User Input
geography = st.selectbox('Geography', onehot_encoder_geo.categories_[0])
gender = st.selectbox('Gender',label_encoder_gender.classes_)
age = st.slider('Age', 18, 92)
balance = st.number_input('Balance')
credit_score = st.number_input('Credit Score')
estimated_salary = st.number_input('Estimated Salary')
tenure = st.slider('Tenure',0,10)
num_of_products = st.slider('Number of Products', 1, 4)
has_credit_card = st.selectbox('Has Credit Card',[0,1])
is_active_member = st.selectbox('Is Active Member', [0,1])

# Prepare the input data

input_data = pd.DataFrame({
    'Credit Score': [credit_score],
    'Gender' : [label_encoder_gender.transform([gender])[0]],
    'Age' : [age],
    'Tenure' : [tenure],
    'Balance' : [balance],
    'Number of Products' : [num_of_products],
    'Has Credit Card' : [has_credit_card],
    'Is Active Member' : [is_active_member],
    'Estimated Salary' : [estimated_salary]
})

input_data.reset_index(drop = True)

# onehot encode 'Geography'

geo_encoded = onehot_encoder_geo.transform([[geography]]).toarray()
geo_encoded_df = pd.DataFrame(geo_encoded, columns = onehot_encoder_geo.get_feature_names_out(['Geography']))
geo_encoded_df.head()

input_data = pd.concat([input_data.reset_index(drop = True),geo_encoded_df], axis = 1)

# Prepare the input data
input_data = pd.DataFrame({
    'CreditScore': [credit_score],
    'Gender' : [label_encoder_gender.transform([gender])[0]],
    'Age' : [age],
    'Tenure' : [tenure],
    'Balance' : [balance],
    'NumOfProducts' : [num_of_products],  # Changed to NumOfProducts
    'HasCrCard' : [has_credit_card],     # Changed to HasCrCard
    'IsActiveMember' : [is_active_member], # Changed to IsActiveMember
    'EstimatedSalary' : [estimated_salary]  # Changed to EstimatedSalary
})

# ... (rest of your code) ...

input_data = pd.concat([input_data.reset_index(drop = True),geo_encoded_df], axis = 1)

# Ensure column order matches the original training data
input_data = input_data[['CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',
       'IsActiveMember', 'EstimatedSalary', 'Geography_France', 'Geography_Germany', 'Geography_Spain']]


# Scale the input data
input_data_scaled = scaler.transform(input_data)

# Preddict churn
prediction = model.predict(input_data_scaled)
prediction_poba = prediction[0][0]

st.write(f'Churn Prediction: {prediction_poba:.2f}')
if prediction_poba > 0.5:
  st.write('The Customer is likely to churn.')
else:
  st.write('The Customer is not likely to churn.')